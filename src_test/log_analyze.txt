[NeMo I 2025-11-26 02:30:17 mixins:189] Tokenizer SentencePieceTokenizer initialized with 8192 tokens
[NeMo I 2025-11-26 02:30:20 features:306] PADDING: 0
[NeMo I 2025-11-26 02:30:24 rnnt_models:226] Using RNNT Loss : tdt
    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}
[NeMo I 2025-11-26 02:30:24 rnnt_models:226] Using RNNT Loss : tdt
    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}
[NeMo I 2025-11-26 02:30:24 rnnt_models:226] Using RNNT Loss : tdt
    Loss tdt_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0, 'durations': [0, 1, 2, 3, 4], 'sigma': 0.02, 'omega': 0.1}
[NeMo I 2025-11-26 02:30:27 save_restore_connector:282] Model EncDecRNNTBPEModel was successfully restored from /root/.cache/huggingface/hub/models--nvidia--parakeet-tdt-0.6b-v3/snapshots/be0d803fd1970eca8627f5467c208118f0f6c171/parakeet-tdt-0.6b-v3.nemo.
EncDecRNNTBPEModel(
  (preprocessor): AudioToMelSpectrogramPreprocessor(
    (featurizer): FilterbankFeatures()
  )
  (encoder): ConformerEncoder(
    (pre_encode): ConvSubsampling(
      (out): Linear(in_features=4096, out_features=1024, bias=True)
      (conv): MaskedConvSequential(
        (0): Conv2d(1, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): ReLU(inplace=True)
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (4): ReLU(inplace=True)
        (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
        (6): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (7): ReLU(inplace=True)
      )
    )
    (pos_enc): RelPositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (layers): ModuleList(
      (0-23): 24 x ConformerLayer(
        (norm_feed_forward1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (feed_forward1): ConformerFeedForward(
          (linear1): Linear(in_features=1024, out_features=4096, bias=False)
          (activation): Swish()
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=4096, out_features=1024, bias=False)
        )
        (norm_conv): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (conv): ConformerConvolution(
          (pointwise_conv1): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,), bias=False)
          (depthwise_conv): CausalConv1D(1024, 1024, kernel_size=(9,), stride=(1,), groups=1024, bias=False)
          (batch_norm): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (activation): Swish()
          (pointwise_conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)
        )
        (norm_self_att): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (self_attn): RelPositionMultiHeadAttention(
          (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
          (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
          (linear_v): Linear(in_features=1024, out_features=1024, bias=False)
          (linear_out): Linear(in_features=1024, out_features=1024, bias=False)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear_pos): Linear(in_features=1024, out_features=1024, bias=False)
        )
        (norm_feed_forward2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (feed_forward2): ConformerFeedForward(
          (linear1): Linear(in_features=1024, out_features=4096, bias=False)
          (activation): Swish()
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=4096, out_features=1024, bias=False)
        )
        (dropout): Dropout(p=0.1, inplace=False)
        (norm_out): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): RNNTDecoder(
    (prediction): ModuleDict(
      (embed): Embedding(8193, 640, padding_idx=8192)
      (dec_rnn): LSTMDropout(
        (lstm): LSTM(640, 640, num_layers=2, dropout=0.2)
        (dropout): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (joint): RNNTJoint(
    (pred): Linear(in_features=640, out_features=640, bias=True)
    (enc): Linear(in_features=1024, out_features=640, bias=True)
    (joint_net): Sequential(
      (0): ReLU(inplace=True)
      (1): Dropout(p=0.2, inplace=False)
      (2): Linear(in_features=640, out_features=8198, bias=True)
    )
    (_loss): RNNTLoss(
      (_loss): TDTLossNumba()
    )
    (_wer): WER()
  )
  (loss): RNNTLoss(
    (_loss): TDTLossNumba()
  )
  (spec_augmentation): SpectrogramAugmentation(
    (spec_augment): SpecAugment()
  )
  (wer): WER()
)
正在註冊 LayerNorm Hooks...
Hooks 已移除。

====================================================================================================
Detailed LayerNorm Activation Statistics
====================================================================================================

--- LayerNorm Statistics for Type: FeedForward (FF) ---
Layer Index     |    Max Abs |    Min Abs |   Mean Abs |      Std Dev | Max Per Dim
---------------------------------------------------------------------------
L0-FF1          |     7.2470 |  0.0000002 |     0.2529 |     0.3815 |       7.2470 | OK
L0-FF2          |    13.2597 |  0.0000019 |     0.4922 |     0.7436 |      13.2597 | OK
L1-FF1          |    16.8886 |  0.0000002 |     0.0713 |     0.3335 |      16.8886 | OK
L1-FF2          |     3.1618 |  0.0000001 |     0.1030 |     0.1958 |       3.1618 | OK
L2-FF1          |     9.9656 |  0.0000002 |     0.0664 |     0.2005 |       9.9656 | OK
L2-FF2          |     2.7733 |  0.0000003 |     0.0900 |     0.1666 |       2.7733 | OK
L3-FF1          |     8.4445 |  0.0000002 |     0.0824 |     0.2162 |       8.4445 | OK
L3-FF2          |     4.3155 |  0.0000007 |     0.1044 |     0.2103 |       4.3155 | OK
L4-FF1          |     6.1596 |  0.0000001 |     0.0900 |     0.2297 |       6.1596 | OK
L4-FF2          |     3.4929 |  0.0000003 |     0.1076 |     0.1985 |       3.4929 | OK
L5-FF1          |     5.7177 |  0.0000001 |     0.0899 |     0.1945 |       5.7177 | OK
L5-FF2          |     3.1482 |  0.0000001 |     0.0989 |     0.1707 |       3.1482 | OK
L6-FF1          |     3.2245 |  0.0000001 |     0.0979 |     0.1553 |       3.2245 | OK
L6-FF2          |     1.7280 |  0.0000004 |     0.0890 |     0.1365 |       1.7280 | OK
L7-FF1          |     5.6914 |  0.0000000 |     0.0974 |     0.1881 |       5.6914 | OK
L7-FF2          |     2.2766 |  0.0000000 |     0.0958 |     0.1535 |       2.2766 | OK
L8-FF1          |     9.1898 |  0.0000002 |     0.0981 |     0.2582 |       9.1898 | OK
L8-FF2          |     3.6166 |  0.0000002 |     0.0926 |     0.1700 |       3.6166 | OK
L9-FF1          |     4.2554 |  0.0000005 |     0.1146 |     0.1977 |       4.2554 | OK
L9-FF2          |     1.8399 |  0.0000000 |     0.1093 |     0.1631 |       1.8399 | OK
L10-FF1         |     2.9753 |  0.0000007 |     0.2264 |     0.3185 |       2.9753 | OK
L10-FF2         |     2.1334 |  0.0000002 |     0.1289 |     0.1910 |       2.1334 | OK
L11-FF1         |     3.6124 |  0.0000005 |     0.1950 |     0.2887 |       3.6124 | OK
L11-FF2         |     1.7193 |  0.0000006 |     0.1482 |     0.1952 |       1.7193 | OK
L12-FF1         |     4.3689 |  0.0000012 |     0.2377 |     0.3703 |       4.3689 | OK
L12-FF2         |     1.1643 |  0.0000005 |     0.1314 |     0.1732 |       1.1643 | OK
L13-FF1         |     3.5950 |  0.0000013 |     0.2647 |     0.3932 |       3.5950 | OK
L13-FF2         |     1.9690 |  0.0000001 |     0.1236 |     0.1758 |       1.9690 | OK
L14-FF1         |     3.3895 |  0.0000003 |     0.2737 |     0.3902 |       3.3895 | OK
L14-FF2         |     1.5682 |  0.0000002 |     0.1133 |     0.1648 |       1.5682 | OK
L15-FF1         |     2.6335 |  0.0000023 |     0.1827 |     0.2528 |       2.6335 | OK
L15-FF2         |     2.2865 |  0.0000001 |     0.1291 |     0.1953 |       2.2865 | OK
L16-FF1         |     2.9040 |  0.0000020 |     0.1530 |     0.2356 |       2.9040 | OK
L16-FF2         |     3.3830 |  0.0000002 |     0.0891 |     0.1666 |       3.3830 | OK
L17-FF1         |     4.9329 |  0.0000002 |     0.0932 |     0.2197 |       4.9329 | OK
L17-FF2         |     2.3744 |  0.0000004 |     0.0881 |     0.1573 |       2.3744 | OK
L18-FF1         |     4.1490 |  0.0000003 |     0.0909 |     0.1976 |       4.1490 | OK
L18-FF2         |     3.4609 |  0.0000027 |     0.0895 |     0.1621 |       3.4609 | OK
L19-FF1         |     2.2889 |  0.0000002 |     0.0835 |     0.1485 |       2.2889 | OK
L19-FF2         |     4.4815 |  0.0000000 |     0.0832 |     0.1769 |       4.4815 | OK
L20-FF1         |     6.6880 |  0.0000002 |     0.0857 |     0.2445 |       6.6880 | OK
L20-FF2         |     6.2561 |  0.0000003 |     0.0900 |     0.2238 |       6.2561 | OK
L21-FF1         |     6.2592 |  0.0000001 |     0.0910 |     0.2511 |       6.2592 | OK
L21-FF2         |     4.9520 |  0.0000000 |     0.0850 |     0.1772 |       4.9520 | OK
L22-FF1         |     2.1916 |  0.0000009 |     0.0852 |     0.1463 |       2.1916 | OK
L22-FF2         |     4.5861 |  0.0000001 |     0.1030 |     0.2144 |       4.5861 | OK
L23-FF1         |     2.9108 |  0.0000004 |     0.0847 |     0.1414 |       2.9108 | OK
L23-FF2         |     3.9192 |  0.0000001 |     0.0923 |     0.1425 |       3.9192 | OK

--- LayerNorm Statistics for Type: Attention (Att) ---
Layer Index     |    Max Abs |    Min Abs |   Mean Abs |      Std Dev | Max Per Dim
---------------------------------------------------------------------------
L0-Att          |     1.3478 |  0.0000000 |     0.0575 |     0.0819 |       1.3478 | DEAD?
L1-Att          |     0.8764 |  0.0000000 |     0.0250 |     0.0355 |       0.8764 | DEAD?
L2-Att          |     0.8354 |  0.0000000 |     0.0275 |     0.0379 |       0.8354 | DEAD?
L3-Att          |     0.6784 |  0.0000000 |     0.0287 |     0.0394 |       0.6784 | DEAD?
L4-Att          |     0.6445 |  0.0000001 |     0.0289 |     0.0399 |       0.6445 | DEAD?
L5-Att          |     0.6803 |  0.0000003 |     0.0288 |     0.0400 |       0.6803 | DEAD?
L6-Att          |     0.5152 |  0.0000001 |     0.0287 |     0.0389 |       0.5152 | DEAD?
L7-Att          |     0.4829 |  0.0000001 |     0.0263 |     0.0359 |       0.4829 | DEAD?
L8-Att          |     0.5479 |  0.0000002 |     0.0274 |     0.0379 |       0.5479 | DEAD?
L9-Att          |     0.3887 |  0.0000002 |     0.0271 |     0.0360 |       0.3887 | DEAD?
L10-Att         |     0.2830 |  0.0000000 |     0.0246 |     0.0319 |       0.2830 | DEAD?
L11-Att         |     0.3244 |  0.0000001 |     0.0249 |     0.0321 |       0.3244 | DEAD?
L12-Att         |     0.3966 |  0.0000000 |     0.0275 |     0.0360 |       0.3966 | DEAD?
L13-Att         |     0.5629 |  0.0000002 |     0.0321 |     0.0418 |       0.5629 | DEAD?
L14-Att         |     0.3717 |  0.0000001 |     0.0312 |     0.0406 |       0.3717 | DEAD?
L15-Att         |     0.3342 |  0.0000000 |     0.0256 |     0.0330 |       0.3342 | DEAD?
L16-Att         |     0.5031 |  0.0000001 |     0.0295 |     0.0404 |       0.5031 | DEAD?
L17-Att         |     0.4104 |  0.0000001 |     0.0204 |     0.0283 |       0.4104 | DEAD?
L18-Att         |     0.2972 |  0.0000000 |     0.0237 |     0.0321 |       0.2972 | DEAD?
L19-Att         |     0.2944 |  0.0000001 |     0.0229 |     0.0315 |       0.2944 | DEAD?
L20-Att         |     0.3133 |  0.0000000 |     0.0269 |     0.0356 |       0.3133 | DEAD?
L21-Att         |     0.4980 |  0.0000001 |     0.0310 |     0.0411 |       0.4980 | DEAD?
L22-Att         |     0.4398 |  0.0000000 |     0.0355 |     0.0464 |       0.4398 | DEAD?
L23-Att         |     0.6085 |  0.0000001 |     0.0378 |     0.0497 |       0.6085 | DEAD?

--- LayerNorm Statistics for Type: Convolution (Conv) ---
Layer Index     |    Max Abs |    Min Abs |   Mean Abs |      Std Dev | Max Per Dim
---------------------------------------------------------------------------
L0-Conv         |     3.6359 |  0.0000006 |     0.0848 |     0.1396 |       3.6359 | OK
L1-Conv         |     3.7101 |  0.0000008 |     0.0752 |     0.1127 |       3.7101 | OK
L2-Conv         |     3.2410 |  0.0000003 |     0.0761 |     0.1127 |       3.2410 | OK
L3-Conv         |     2.5900 |  0.0000003 |     0.0772 |     0.1121 |       2.5900 | OK
L4-Conv         |     2.7855 |  0.0000000 |     0.0752 |     0.1110 |       2.7855 | OK
L5-Conv         |     2.6441 |  0.0000001 |     0.0739 |     0.1109 |       2.6441 | OK
L6-Conv         |     2.5246 |  0.0000003 |     0.0774 |     0.1128 |       2.5246 | OK
L7-Conv         |     2.6966 |  0.0000003 |     0.0818 |     0.1184 |       2.6966 | OK
L8-Conv         |     2.4256 |  0.0000000 |     0.0790 |     0.1127 |       2.4256 | OK
L9-Conv         |     2.1136 |  0.0000002 |     0.0875 |     0.1201 |       2.1136 | OK
L10-Conv        |     1.5087 |  0.0000001 |     0.0985 |     0.1309 |       1.5087 | OK
L11-Conv        |     1.2611 |  0.0000000 |     0.1018 |     0.1323 |       1.2611 | OK
L12-Conv        |     1.4598 |  0.0000000 |     0.1038 |     0.1344 |       1.4598 | OK
L13-Conv        |     2.2560 |  0.0000003 |     0.0961 |     0.1267 |       2.2560 | OK
L14-Conv        |     1.7722 |  0.0000003 |     0.0934 |     0.1240 |       1.7722 | OK
L15-Conv        |     1.7708 |  0.0000001 |     0.0896 |     0.1224 |       1.7708 | OK
L16-Conv        |     1.9039 |  0.0000004 |     0.0874 |     0.1232 |       1.9039 | OK
L17-Conv        |     2.0339 |  0.0000000 |     0.0812 |     0.1192 |       2.0339 | OK
L18-Conv        |     1.5215 |  0.0000002 |     0.0777 |     0.1141 |       1.5215 | OK
L19-Conv        |     1.2284 |  0.0000001 |     0.0743 |     0.1047 |       1.2284 | OK
L20-Conv        |     1.1172 |  0.0000006 |     0.0805 |     0.1090 |       1.1172 | OK
L21-Conv        |     1.1358 |  0.0000001 |     0.0783 |     0.1068 |       1.1358 | OK
L22-Conv        |     0.9882 |  0.0000002 |     0.0792 |     0.1058 |       0.9882 | OK
L23-Conv        |     1.0234 |  0.0000004 |     0.0635 |     0.0856 |       1.0234 | DEAD?

--- LayerNorm Statistics for Type: Output (Out) ---
Layer Index     |    Max Abs |    Min Abs |   Mean Abs |      Std Dev | Max Per Dim
---------------------------------------------------------------------------
L0-Out          |   198.7996 |  0.0000041 |     6.4659 |    10.4218 |     198.7996 | ⚠️ SPIKE
L1-Out          |   227.6641 |  0.0000328 |     6.1707 |    10.9441 |     227.6641 | ⚠️ SPIKE
L2-Out          |   200.1748 |  0.0000135 |     5.8996 |    10.2878 |     200.1748 | ⚠️ SPIKE
L3-Out          |   194.8074 |  0.0000152 |     5.6678 |    10.1346 |     194.8074 | ⚠️ SPIKE
L4-Out          |   204.8507 |  0.0000071 |     5.6180 |    10.5810 |     204.8507 | ⚠️ SPIKE
L5-Out          |   192.0014 |  0.0000052 |     5.4860 |    10.6130 |     192.0014 | ⚠️ SPIKE
L6-Out          |   182.0726 |  0.0000075 |     5.3108 |    10.1910 |     182.0726 | ⚠️ SPIKE
L7-Out          |   207.1533 |  0.0000386 |     5.3708 |    10.0965 |     207.1533 | ⚠️ SPIKE
L8-Out          |   256.6069 |  0.0000018 |     5.4498 |    10.2553 |     256.6069 | ⚠️ SPIKE
L9-Out          |   158.2840 |  0.0000058 |     5.8968 |     9.9956 |     158.2840 | ⚠️ SPIKE
L10-Out         |   160.2970 |  0.0000049 |     6.2077 |    10.3404 |     160.2970 | ⚠️ SPIKE
L11-Out         |   198.9006 |  0.0000407 |     6.2908 |    10.6028 |     198.9006 | ⚠️ SPIKE
L12-Out         |   473.8062 |  0.0000676 |     6.1596 |    11.2045 |     473.8062 | ⚠️ SPIKE
L13-Out         |   455.3306 |  0.0000023 |     5.7927 |    10.9163 |     455.3306 | ⚠️ SPIKE
L14-Out         |   333.5745 |  0.0000009 |     5.5811 |    10.1301 |     333.5745 | ⚠️ SPIKE
L15-Out         |   487.2500 |  0.0000118 |     5.8338 |    11.6443 |     487.2500 | ⚠️ SPIKE
L16-Out         |   176.6914 |  0.0000405 |     5.9991 |    10.6642 |     176.6914 | ⚠️ SPIKE
L17-Out         |   272.2701 |  0.0000021 |     5.8801 |    11.6564 |     272.2701 | ⚠️ SPIKE
L18-Out         |   301.1159 |  0.0000019 |     5.4166 |    11.6762 |     301.1159 | ⚠️ SPIKE
L19-Out         |   282.5050 |  0.0000042 |     5.3527 |    10.7416 |     282.5050 | ⚠️ SPIKE
L20-Out         |   265.3781 |  0.0000534 |     5.5461 |    10.1872 |     265.3781 | ⚠️ SPIKE
L21-Out         |   266.8427 |  0.0000071 |     5.0988 |     9.0006 |     266.8427 | ⚠️ SPIKE
L22-Out         |   504.4389 |  0.0000145 |     5.8076 |    12.2979 |     504.4389 | ⚠️ SPIKE
L23-Out         |     0.1844 |  0.0000000 |     0.0148 |     0.0199 |       0.1844 | DEAD?

====================================================================================================
診斷參考：
1. SPIKE: 'Max Per Dim' > 20.0，特徵爆炸。
2. DEAD?: 'Std Dev' < 0.1，模組可能不活躍或死亡。
